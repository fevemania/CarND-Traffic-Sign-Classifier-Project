{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Project: Build a Traffic Sign Recognition Classifier\n",
    "\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Step 0: Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = \"train.p\"\n",
    "validation_file= \"valid.p\"\n",
    "testing_file = \"test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Summary & Exploration\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. **THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Provide a Basic Summary of the Data Set Using Python, Numpy and/or Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples =  34799\n",
      "Number of validation examples =  4410\n",
      "Number of testing examples =  12630\n",
      "Image data shape =  (32, 32, 3)\n",
      "Number of classes =  43\n"
     ]
    }
   ],
   "source": [
    "# First make sure the volume of data is correct\n",
    "assert(len(X_train) == len(y_train))\n",
    "assert(len(X_valid) == len(y_valid))\n",
    "assert(len(X_test) == len(y_test))\n",
    "\n",
    "n_train = len(y_train)\n",
    "n_validation = len(y_valid)\n",
    "n_test = len(y_test)\n",
    "\n",
    "image_shape = X_train[0].shape\n",
    "n_classes = len(set(y_train))\n",
    "\n",
    "print(\"Number of training examples = \", n_train)\n",
    "print(\"Number of validation examples = \", n_validation)\n",
    "print(\"Number of testing examples = \", n_test)\n",
    "print(\"Image data shape = \", image_shape)\n",
    "print(\"Number of classes = \", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Include an Exploratory Visualization of the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Visualize the German Traffic Signs Dataset using the pickled file(s). This is open ended, suggestions include: plotting traffic sign images, plotting the count of each sign, etc. \n",
    "\n",
    "The [Matplotlib](http://matplotlib.org/) [examples](http://matplotlib.org/examples/index.html) and [gallery](http://matplotlib.org/gallery.html) pages are a great resource for doing visualizations in Python.\n",
    "\n",
    "**NOTE:** It's recommended you start with something simple first. If you wish to do more, come back to it after you've completed the rest of the sections. It can be interesting to look at the distribution of classes in the training, validation and test set. Is the distribution the same? Are there more examples of some classes than others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Look at the distribution of classes in the training, validation and test set\n",
    "#\n",
    "# result: found that the data is imbalanced\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "print(\"Distribution of Training set:\")\n",
    "\n",
    "train_label_classes, num_of_train_labels = zip(*Counter(y_train).items())\n",
    "\n",
    "indexes = np.arange(len(train_label_classes))\n",
    "width = 1\n",
    "\n",
    "plt.bar(indexes, num_of_train_labels, width)\n",
    "plt.show()\n",
    "\n",
    "print(\"Distribution of Validation set:\")\n",
    "\n",
    "valid_label_classes, num_of_valid_labels = zip(*Counter(y_valid).items())\n",
    "plt.bar(indexes, num_of_valid_labels, width)\n",
    "plt.show()\n",
    "\n",
    "print(\"Distribution of Test set:\")\n",
    "test_label_classes, num_of_test_labels = zip(*Counter(y_test).items())\n",
    "plt.bar(indexes, num_of_test_labels, width)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plot a random image\n",
    "import random\n",
    "\n",
    "\n",
    "index = random.randint(0, len(X_train))\n",
    "\n",
    "print(\"random index: \", index)\n",
    "print(\"image original size: \", train['sizes'][index][0], 'x', train['sizes'][index][1])\n",
    "print(\"sign position in original image: \", train['coords'][index])\n",
    "\n",
    "print(\"type of single image in X_train: \", type(X_train[index]))\n",
    "print(\"type of single pixel intensity of one image in X_train: \", X_train[index].dtype)\n",
    "\n",
    "coords = train['coords'][index]\n",
    "\n",
    "# Can't use mpimg.imread directly read image from numpy.ndarray\n",
    "# -> TypeError: Object does not appear to be a 8-bit string path or a Python file-like object\n",
    "# ex. image = mpimg.imread(X_train[10000])\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "\n",
    "# interpolation='nearest' simply display the image without try to interpolate betwen pixels \n",
    "# if the display resolution is not the same as the image resolution (which is most often the case)\n",
    "plt.imshow(X_train[index], interpolation='nearest')\n",
    "print()\n",
    "print(\"label of the picture: \", y_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plot a bunch of random images from all 43 classifications\n",
    "\n",
    "# get the idea from the blog of Alex Staravoitau\n",
    "# https://navoshta.com/traffic-signs-classification/\n",
    "\n",
    "from pandas.io.parsers import read_csv\n",
    "\n",
    "signnames = read_csv(\"signnames.csv\").values[:, 1]\n",
    "sign_classes, class_indices, class_counts = np.unique(y_train, return_index = True, return_counts = True)\n",
    "\n",
    "col_width = max(len(name) for name in signnames)\n",
    "\n",
    "for sign_class, sign_index, sign_count in zip(sign_classes, class_indices, class_counts):\n",
    "    print(\"Class {}: {:<{}}  {} samples\".format(sign_class, signnames[sign_class], col_width, sign_count))\n",
    "    fig = plt.figure(figsize = (10, 1))\n",
    "    random_indices = random.sample(range(sign_index, sign_index + sign_count), 10)\n",
    "    for i in range(10):\n",
    "        axis = fig.add_subplot(1, 10, i + 1, xticks=[], yticks=[])\n",
    "        axis.set_title(str(random_indices[i]))        \n",
    "        axis.imshow(X_train[random_indices[i]])\n",
    "    plt.show()\n",
    "    print(\"--------------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Observed Results\n",
    "\n",
    "From above, we know that the dataset we use is **imbalanced** and has images that **differ significantly in terms of contrast and brightness**\n",
    "\n",
    "Because of such an imbalanced data, the great accuracy may skew in favor of few classes, and the result turned out to be a lie.\n",
    "\n",
    "> According to [\"8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset\"](http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/), the article in the blog of  Dr. Jason Brownlee. \n",
    "\n",
    "> It's a good idea to **get more data** to **reduce the affect of imbalance**.\n",
    "\n",
    "But before doing things like flipping, rotating, shifting, it would be better to tackle brightness problem first.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "--- \n",
    "## Step2. Pre-process the Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "###  Tackle Brightness Problem\n",
    "\n",
    "From the paper writed by [Pierre Sermanet and Yann LeCun](http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf), It tackle contrast problem by converting image from RGB to YUV colorspace. \n",
    "\n",
    "The Y channel is then preprocessed with global and local contrast normalization while U and V channels are left unchanged\n",
    "\n",
    "Also the article implicitly reveal that the result about using grayscale images to classify bacically eqauls to RGB images.\n",
    "\n",
    "\n",
    "From [this post](https://read01.com/0MOGzg.html). I got the idea that the importance of YUV colorspace is \n",
    "that it can seperate luminance signal (Y) and chrominance signal (U, V). So, if we only take Y component, the image convert to grayscale, and fortunately that's what Opencv function cv2.RGB2GRAY() exactly does for us.\n",
    "\n",
    "(YUV色彩空間的重要性是它的亮度信號Y和色度信號U、V是分離的, 如果只有Y信號分量而沒有U、V分量， 那麼這樣表示的圖就是黑白灰度圖。)\n",
    "\n",
    "    \n",
    "> From Wiki of YUV,\n",
    "\n",
    "> + Y = 0.299 * R + 0.587 * G + 0.114 * B\n",
    "> + U = 0.492 * (B - Y)\n",
    "> + V = 0.877 * (R - Y)\n",
    "\n",
    "After converting to grayscale, I apply local histogram equalization to stretch out the contrast of the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "def grayscale_and_adaptive_histogram_equalization(dataset):\n",
    "    new_dataset = []\n",
    "    for img in dataset:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        # Apply local histogram equalization\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        img = clahe.apply(img)\n",
    "        new_dataset.append(img)\n",
    "    return np.array(new_dataset).reshape(dataset.shape[0], dataset.shape[1], dataset.shape[2], 1)\n",
    "\n",
    "X_train = grayscale_and_adaptive_histogram_equalization(X_train)\n",
    "X_valid = grayscale_and_adaptive_histogram_equalization(X_valid)\n",
    "X_test = grayscale_and_adaptive_histogram_equalization(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pandas.io.parsers import read_csv\n",
    "import random\n",
    "\n",
    "signnames = read_csv(\"signnames.csv\").values[:, 1]\n",
    "sign_classes, class_indices, class_counts = np.unique(y_train, return_index = True, return_counts = True)\n",
    "\n",
    "col_width = max(len(name) for name in signnames)\n",
    "\n",
    "\n",
    "for sign_class, sign_index, sign_count in zip(sign_classes, class_indices, class_counts):\n",
    "    print(\"Class {}: {:<{}}  {} samples\".format(sign_class, signnames[sign_class], col_width, sign_count))\n",
    "    fig = plt.figure(figsize = (10, 1))\n",
    "    random_indices = random.sample(range(sign_index, sign_index + sign_count), 10)\n",
    "    print(\"index of images: \")\n",
    "    for i in range(10):\n",
    "        axis = fig.add_subplot(1, 10, i + 1, xticks=[], yticks=[])\n",
    "        axis.set_title(str(random_indices[i]))\n",
    "        axis.imshow(X_train[random_indices[i]].squeeze(), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    print(\"--------------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Minimally, the image data should be normalized so that the data has mean zero and equal variance. For image data, `(pixel - 128)/ 128` is a quick way to approximately normalize the data and can be used in this project. \n",
    "\n",
    "Other pre-processing steps are optional. You can try different techniques to see if it improves performance. \n",
    "\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flipping \n",
    "(This part is inspired by [Alex Staravoitau](https://navoshta.com/traffic-signs-classification/]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def flip_extend(X, y):\n",
    "    # Classes of signs that, when flipped horizontally, should still be classified as the same class\n",
    "    self_flippable_horizontally = np.array([11, 12, 13, 15, 17, 18, 22, 26, 30, 35])\n",
    "    \n",
    "    # Classes of signs that, when flipped vertically, should still be classified as the same class\n",
    "    self_flippable_vertically = np.array([1, 5, 12, 15, 17])\n",
    "    \n",
    "    # Classes of signs that, when flipped horizontally and then vertically, should still be classified as the same class\n",
    "    self_flippable_both = np.array([32, 40])\n",
    "    \n",
    "    # Classes of signs that, when flipped horizontally, would still be meaningful, but should be classified as some other class\n",
    "    cross_flippable = np.array([\n",
    "        [19, 20], \n",
    "        [33, 34], \n",
    "        [36, 37], \n",
    "        [38, 39],\n",
    "        [20, 19], \n",
    "        [34, 33], \n",
    "        [37, 36], \n",
    "        [39, 38],   \n",
    "    ])\n",
    "    num_classes = 43\n",
    "    \n",
    "    X_extended = np.empty([0, X.shape[1], X.shape[2], X.shape[3]], dtype = X.dtype)\n",
    "    y_extended = np.empty([0], dtype = 'uint8')\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        # First copy existing data for this class\n",
    "        X_extended = np.append(X_extended, X[y == c], axis = 0)\n",
    "    \n",
    "        # If we can flip images of this class horizontally and they would still belong to said class...\n",
    "        if c in self_flippable_horizontally:\n",
    "            # ...Copy their flipped versions into extended array.\n",
    "            X_extended = np.append(X_extended, X[y == c][:, :, ::-1, :], axis = 0)\n",
    "            \n",
    "        # If we can flip images of this class horizontally and they would belong to other class...\n",
    "        if c in cross_flippable[:, 0]:\n",
    "            # ...Copy flipped images of that other class to the extended array.\n",
    "            flip_class = cross_flippable[cross_flippable[:, 0] == c][0][1]\n",
    "            X_extended = np.append(X_extended, X[y == flip_class][:, :, ::-1, :], axis = 0)\n",
    "            \n",
    "        # Fill labels for added images set to current class.\n",
    "        y_extended = np.append(y_extended, np.full((X_extended.shape[0] - y_extended.shape[0]), c))\n",
    "        \n",
    "        # If we can flip images of this class vertically and they would still belong to said class...\n",
    "        if c in self_flippable_vertically:\n",
    "            # ...Copy their flipped versions into extended array.\n",
    "            X_extended = np.append(X_extended, X_extended[y_extended == c][:, ::-1, :, :], axis = 0)\n",
    "            \n",
    "        # Fill labels for added images set to current class.\n",
    "        y_extended = np.append(y_extended, np.full((X_extended.shape[0] - y_extended.shape[0]), c))\n",
    "        \n",
    "        # If we can flip images of this class horizontally AND vertically and they would still belong to said class...\n",
    "        if c in self_flippable_both:\n",
    "            # ...Copy their flipped versions into extended array.\n",
    "            X_extended = np.append(X_extended, X_extended[y_extended == c][:, ::-1, ::-1, :], axis = 0)\n",
    "            \n",
    "        # Fill labels for added images set to current class.\n",
    "        y_extended = np.append(y_extended, np.full((X_extended.shape[0] - y_extended.shape[0]), c))\n",
    "    \n",
    "    return (X_extended, y_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = flip_extend(X_train, y_train)\n",
    "# from int64 to uint8\n",
    "y_train = y_train.astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Rotate and Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_rotate_and_shift = []\n",
    "y_rotate_and_shift = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=2, width_shift_range=0.1, height_shift_range=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "random_acess_size = 10\n",
    "upperbound = len(X_train)//10*10\n",
    "\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=random_acess_size):\n",
    "    for i in range(0, random_acess_size):\n",
    "        X_rotate_and_shift.append(X_batch[i])\n",
    "        y_rotate_and_shift.append(y_batch[i])   \n",
    "    \n",
    "    count += 10\n",
    "    if count >= upperbound:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_rotate_and_shift = np.array(X_rotate_and_shift).astype('uint8')\n",
    "y_rotate_and_shift = np.array(y_rotate_and_shift).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train, X_rotate_and_shift))\n",
    "y_train = np.concatenate((y_train, y_rotate_and_shift))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### After Flipping, Rotating, and Shifting, the total train data comes to 119568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Normalize (In order to make compute efficiently and precisely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(X_train.dtype)\n",
    "print(y_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# # First we should change type of features from int8 to float32 in order to do normalize\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_valid = X_valid.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "# Then set flags for feature engineering tasks I am going to do.  This will prevent me from skipping an important step.\n",
    "is_features_normal = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def normalize_dataset(image_data):\n",
    "    \"\"\"\n",
    "    Normalize the image data\n",
    "    :param image_data: The image data to be normalized\n",
    "    :return: Normalized image data\n",
    "    \"\"\"    \n",
    "    # +, -, *, / in np.array will automatically broadcast to every element\n",
    "    return (image_data - 128.)/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not is_features_normal:\n",
    "    print(\"Normalizing ...\")\n",
    "    X_train = normalize_dataset(X_train)\n",
    "    X_valid = normalize_dataset(X_valid)\n",
    "    X_test = normalize_dataset(X_test)\n",
    "    is_features_normal = True\n",
    "\n",
    "print(\"Finish Normalization !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Make sure we do Normalization before training the model\n",
    "\n",
    "assert is_features_normal, 'You skipped the step to normalize the features'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step3. Save the Preprocessed Data into Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# store the preprocessing data to one pickle\n",
    "\n",
    "import os\n",
    "\n",
    "# Save the data after preprocessing step for easy access\n",
    "pickle_file = 'traffic.p'\n",
    "# if not os.path.isfile(pickle_file):\n",
    "print('Saving data to pickle file...')\n",
    "try:\n",
    "    with open(pickle_file, 'wb') as pfile:\n",
    "        pickle.dump(\n",
    "            {\n",
    "                'X_train': X_train,\n",
    "                'y_train': y_train,\n",
    "                'X_valid': X_valid,\n",
    "                'y_valid': y_valid,\n",
    "                'X_test': X_test,\n",
    "                'y_test': y_test,\n",
    "            },\n",
    "            pfile, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, \":\", e)\n",
    "    raise\n",
    "\n",
    "print('Data cached in pickle file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step4. Now, we can reload pickle data whenever we restart the jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load the modules\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "# Reload the data\n",
    "pickle_file = 'traffic.p'\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  pickle_data = pickle.load(f)\n",
    "  X_train = pickle_data['X_train']\n",
    "  y_train = pickle_data['y_train']\n",
    "  X_valid = pickle_data['X_valid']\n",
    "  y_valid = pickle_data['y_valid']\n",
    "  X_test = pickle_data['X_test']\n",
    "  y_test = pickle_data['y_test']\n",
    "  del pickle_data  # Free up memory\n",
    "\n",
    "\n",
    "print('Data and modules loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check out the dataset\n",
    "\n",
    "# First make sure the volume of data is correct\n",
    "assert(len(X_train) == len(y_train))\n",
    "assert(len(X_valid) == len(y_valid))\n",
    "assert(len(X_test) == len(y_test))\n",
    "\n",
    "\n",
    "n_train = len(y_train)\n",
    "n_validation = len(y_valid)\n",
    "n_test = len(y_test)\n",
    "\n",
    "image_shape = X_train[0].shape\n",
    "n_classes = len(set(y_train))\n",
    "\n",
    "print(\"Number of training examples = \", n_train)\n",
    "print(\"Number of validation examples = \", n_validation)\n",
    "print(\"Number of testing examples = \", n_test)\n",
    "print(\"Image data shape = \", image_shape)\n",
    "print(\"Number of classes = \", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(X_train.dtype)\n",
    "print(y_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step5. Develop Model Architecture\n",
    "\n",
    "This part is inspired by [Alex Staravoitau](https://navoshta.com/traffic-signs-classification/]), but rather than 5x5 filter, it was 3x3 makes my validation more precisely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Define your architecture here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x, keep_prob_conv1, keep_prob_conv2, keep_prob_conv3, keep_prob_fc1):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # P.S.\n",
    "    # strides = [batch, height, width, depth]\n",
    "    # shape = (height, width, input_depth, output_depth)\n",
    "    \n",
    "    # Layer 1: Convolutional. Input = 32x32x1. Output = 32x32x32.\n",
    "    \n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 1, 32), mean = mu, stddev = sigma)) \n",
    "    conv1_b = tf.Variable(tf.zeros(32))\n",
    "    conv1 = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='SAME') + conv1_b \n",
    "    \n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # Layer 1: Pooling. Input = 32x32x32. Output = 16x16x32.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    # add Dropout\n",
    "    conv1 = tf.nn.dropout(conv1, keep_prob_conv1)\n",
    "    \n",
    "    # Layer 2: Convolutional. Input = 16x16x32. Output = 16x16x64.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 32, 64), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(64))\n",
    "    conv2 = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='SAME') + conv2_b\n",
    "    \n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # Layer 2: Pooling. Input = 16x16x64. Output = 8x8x64.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    # add Dropout    \n",
    "    conv2 = tf.nn.dropout(conv2, keep_prob_conv2)\n",
    "     \n",
    "    # Layer 3: Convolutional. Input = 8x8x64, Output = 8x8x128.\n",
    "    conv3_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 64, 128), mean = mu, stddev = sigma))\n",
    "    conv3_b = tf.Variable(tf.zeros(128))\n",
    "    conv3 = tf.nn.conv2d(conv2, conv3_W, strides=[1, 1, 1, 1], padding='SAME') + conv3_b\n",
    "    \n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "    \n",
    "    # Layer 3: Pooling. Input = 8x8x128. Output = 4x4x128.\n",
    "    conv3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    # add Dropout    \n",
    "    conv3 = tf.nn.dropout(conv3, keep_prob_conv3)\n",
    "    \n",
    "    # Using Multi-Scaling to let fully-connect get the information from all of conv layers\n",
    "    \n",
    "    # OUTPUT of layer 1\n",
    "    # Input = 16x16x16, Output = 4*4*32\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding='SAME')\n",
    "    fc0_from_conv1 = flatten(pool1)\n",
    "    \n",
    "    # OUTPUT of layer2\n",
    "    # Input = 8x8x32, Output = 4x4x64\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    fc0_from_conv2 = flatten(pool2)\n",
    "\n",
    "    # OUTPUT of layer3\n",
    "\n",
    "    fc0_from_conv3 = flatten(conv3)\n",
    "    \n",
    "    # Flatten. Input = 4*4* (32+64+128) = 3584\n",
    "    fc0 = tf.concat([fc0_from_conv1, fc0_from_conv2, fc0_from_conv3], 1)\n",
    "    \n",
    "    # TODO: Layer 3: Fully Connected. Input = 3584. Output = 1024.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(3584, 1024), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(1024))\n",
    "    fc1 = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # TODO: Activation.\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "    # add Dropout    \n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob_fc1)\n",
    "    \n",
    "    # TODO: Layer 4: Fully Connected. Input = 1024. Output = 84.\n",
    "    fc2_W = tf.Variable(tf.truncated_normal(shape=(1024, 43), mean = mu, stddev = sigma))\n",
    "    fc2_b = tf.Variable(tf.zeros(43))\n",
    "    logits = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.uint8, (None))\n",
    "one_hot_y = tf.one_hot(y, 43)\n",
    "keep_prob_conv1 = tf.placeholder(tf.float32)\n",
    "keep_prob_conv2 = tf.placeholder(tf.float32)\n",
    "keep_prob_conv3 = tf.placeholder(tf.float32)\n",
    "keep_prob_fc1 = tf.placeholder(tf.float32)\n",
    "\n",
    "rate = 0.001\n",
    "\n",
    "logits = LeNet(x, keep_prob_conv1, keep_prob_conv2, keep_prob_conv3, keep_prob_fc1)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob_conv1: 1.0, keep_prob_conv2: 1.0,\n",
    "                                                   keep_prob_conv3: 1.0, keep_prob_fc1: 1.0})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step6. Train and Validate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A validation set can be used to assess how well the model is performing. A low accuracy on the training and validation sets imply underfitting. A high accuracy on the training set but low accuracy on the validation set implies overfitting.\n",
    "\n",
    "To make the converge more faster, i use the high initial learning rate = 0.001\n",
    "And check if the valid error stop decreasing per epoch, if yes, multiply rate by 0.1\n",
    "\n",
    "To prevent from overfitting, the model introducing dropout on both convolutional and fully connect layer, by lots of test and error, slight drop out in convolutional layer helps to reach better accuracy.\n",
    "\n",
    "Also I introducing early stopping to prevent from overfitting. The stop criterion I use is to save the weighted combination wich makes highest valid accuracy and lowest valid error during training. If the following 20 epochs couldn't update the last record, then stop training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "# reset learning rate\n",
    "rate = 0.001\n",
    "\n",
    "cnt_within_each_epoch = 1\n",
    "\n",
    "last_valid_error = float('Inf')\n",
    "\n",
    "highest_valid_accuracy = 0\n",
    "lowest_valid_loss = float('Inf')\n",
    "\n",
    "early_stop_cnt = 0\n",
    "is_early_stop = False\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob_conv1: 0.9, keep_prob_conv2: 0.8,\n",
    "                                                   keep_prob_conv3: 0.8, keep_prob_fc1: 0.5})\n",
    "\n",
    "        new_valid_error = sess.run(loss_operation, feed_dict={x: X_valid, y: y_valid, keep_prob_conv1: 1.0, keep_prob_conv2: 1.0,\n",
    "                                                   keep_prob_conv3: 1.0, keep_prob_fc1: 1.0})\n",
    "\n",
    "        if last_valid_error < new_valid_error:\n",
    "            rate *= 0.1\n",
    "        last_valid_error = new_valid_error\n",
    "        \n",
    "#         train_accuracy = evaluate(X_train, y_train)\n",
    "#         print(\"EPOCH {} ...\".format(i+1))\n",
    "#         print(\"Train Accuracy = {:.3f}\".format(train_accuracy))\n",
    "#         print(\"Training Error = {}\".format(sess.run(loss_operation, feed_dict={x: X_train, y: y_train, keep_prob_conv1: 1.0, keep_prob_conv2: 1.0,\n",
    "#                                                    keep_prob_conv3: 1.0, keep_prob_fc1: 1.0})))\n",
    "#         print()\n",
    "#         if (i + 1) % 5 == 0:\n",
    "        \n",
    "        valid_accuracy = evaluate(X_valid, y_valid)\n",
    "    \n",
    "    \n",
    "        if (valid_accuracy > highest_valid_accuracy and lowest_valid_loss > new_valid_error):\n",
    "            highest_valid_accuracy = valid_accuracy\n",
    "            lowest_valid_loss = new_valid_error\n",
    "            early_stop_cnt = 0\n",
    "            saver.save(sess, 'record/lenet')\n",
    "            print(\"Model saved\")\n",
    "        else:\n",
    "            early_stop_cnt += 1\n",
    "    \n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(valid_accuracy))\n",
    "        print(\"Cross Validation Error = {}\".format(new_valid_error))\n",
    "        print()\n",
    "        \n",
    "        if early_stop_cnt == 20:\n",
    "            is_early_stop = True\n",
    "            break\n",
    "        \n",
    "    if not is_early_stop and valid_accuracy > highest_valid_accuracy and lowest_valid_loss > new_valid_error:\n",
    "        saver.save(sess, 'record/lenet')\n",
    "        print(\"Model saved\")\n",
    "    else:\n",
    "        print(\"Early Stopping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## Step 3: Test a Model on New Images\n",
    "\n",
    "To give yourself more insight into how your model is working, download at least five pictures of German traffic signs from the web and use your model to predict the traffic sign type.\n",
    "\n",
    "You may find `signnames.csv` useful as it contains mappings from the class id (integer) to the actual sign name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Predict the Sign Type for Each Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Run the predictions here and use the model to output the prediction for each image.\n",
    "### Make sure to pre-process the images with the same pre-processing pipeline used earlier.\n",
    "### Feel free to use as many code cells as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(os.listdir('images/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_image_set = np.array([28, 12, 4, 38, 25, 7]).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAABCCAYAAABKKV0QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7a0093e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nURC2iygI20UUhO0i/h/JfMjFhjSYDAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7a0150dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAABCCAYAAABKKV0QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7a00794a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nr59F2LyOOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7a004f4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAABCCAYAAABKKV0QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa798782160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7ACpxewAqcXsAKnF7ADp/wEZO573pXg9gQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa798593e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAABCCAYAAABKKV0QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7985fce48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7a014bba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAABCCAYAAABKKV0QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7a015c390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nI0RN2BGiJuwIURN2hKgJO0LUhB0h/h/5I727z4iDbQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7986c8978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAABCCAYAAABKKV0QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7987a9f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa798772748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Load the images , show them and preprocess then.\n",
    "# every image should do grayscale, local histogram equalize and normalize\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "X_image_set = []\n",
    "\n",
    "i = 0\n",
    "for img_name in os.listdir('images/'):\n",
    "    img = cv2.imread('images/' + img_name)\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    img = clahe.apply(img)\n",
    "    X_image_set.append(img)\n",
    "\n",
    "num_of_image_set = len(X_image_set)\n",
    "    \n",
    "X_image_set = np.array(X_image_set).reshape(num_of_image_set, 32, 32, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test_image_set[0] = cv2.cvtColor(test_image_set[0], cv2.COLOR_RGB2GRAY)\n",
    "# in order to use local equalization, we need to first convert image dtype to uint8\n",
    "\n",
    "X_image_set = X_image_set.astype('float32')\n",
    "X_image_set = normalize_dataset(X_image_set)\n",
    "print(X_image_set.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(X_image_set[5].squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "\n",
    "predictions = tf.nn.softmax(logits)\n",
    "top_k_predictions = tf.nn.top_k(predictions, k)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tf.train.Saver().restore(sess, tf.train.latest_checkpoint('record'))\n",
    "\n",
    "    \n",
    "    p = sess.run(top_k_predictions, feed_dict = {\n",
    "        x : X_image_set, keep_prob_conv1: 1.0, keep_prob_conv2: 1.0, keep_prob_conv3: 1.0, keep_prob_fc1: 1.0\n",
    "        }\n",
    "    )\n",
    "\n",
    "# predictions = predictions[1][:, np.argmax(predictions[0], 1)][:, 0].astype(int)\n",
    "#     print(\"yes Accuracy = {:.3f}\".format(yes_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pandas.io.parsers import read_csv\n",
    "\n",
    "signnames = read_csv(\"signnames.csv\").values[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictions = p[1][:, np.argmax(p[0], 1)][:, 0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "p[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_image_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "\n",
    "\n",
    "pos = arange(5)+.5    # the bar centers on the y axis\n",
    "\n",
    "for i in range(len(X_image_set)):\n",
    "\n",
    "    val = p[0][i]         # the bar lengths\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 3))\n",
    "    \n",
    "    print(\"Class {}: {}\".format(y_image_set[i], signnames[y_image_set[i]]))\n",
    "    axes[0].imshow(X_image_set[i].squeeze(), cmap='gray')\n",
    "    axes[1].barh(pos,val, align='center')\n",
    "    axes[1].set_yticks(pos)\n",
    "    axes[1].set_yticklabels(signnames[p[1][i]])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load model variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('record'))\n",
    "\n",
    "    train_accuracy = evaluate(X_train, y_train)\n",
    "    print(\"Train Accuracy = {:.3f}\".format(train_accuracy))\n",
    "    \n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Analyze Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Calculate the accuracy for these 5 new images. \n",
    "### For example, if the model predicted 1 out of 5 signs correctly, it's 20% accurate on these new images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Top 5 Softmax Probabilities For Each Image Found on the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For each of the new images, print out the model's softmax probabilities to show the **certainty** of the model's predictions (limit the output to the top 5 probabilities for each image). [`tf.nn.top_k`](https://www.tensorflow.org/versions/r0.12/api_docs/python/nn.html#top_k) could prove helpful here. \n",
    "\n",
    "The example below demonstrates how tf.nn.top_k can be used to find the top k predictions for each image.\n",
    "\n",
    "`tf.nn.top_k` will return the values and indices (class ids) of the top k predictions. So if k=3, for each sign, it'll return the 3 largest probabilities (out of a possible 43) and the correspoding class ids.\n",
    "\n",
    "Take this numpy array as an example. The values in the array represent predictions. The array contains softmax probabilities for five candidate images with six possible classes. `tk.nn.top_k` is used to choose the three classes with the highest probability:\n",
    "\n",
    "```\n",
    "# (5, 6) array\n",
    "a = np.array([[ 0.24879643,  0.07032244,  0.12641572,  0.34763842,  0.07893497,\n",
    "         0.12789202],\n",
    "       [ 0.28086119,  0.27569815,  0.08594638,  0.0178669 ,  0.18063401,\n",
    "         0.15899337],\n",
    "       [ 0.26076848,  0.23664738,  0.08020603,  0.07001922,  0.1134371 ,\n",
    "         0.23892179],\n",
    "       [ 0.11943333,  0.29198961,  0.02605103,  0.26234032,  0.1351348 ,\n",
    "         0.16505091],\n",
    "       [ 0.09561176,  0.34396535,  0.0643941 ,  0.16240774,  0.24206137,\n",
    "         0.09155967]])\n",
    "```\n",
    "\n",
    "Running it through `sess.run(tf.nn.top_k(tf.constant(a), k=3))` produces:\n",
    "\n",
    "```\n",
    "TopKV2(values=array([[ 0.34763842,  0.24879643,  0.12789202],\n",
    "       [ 0.28086119,  0.27569815,  0.18063401],\n",
    "       [ 0.26076848,  0.23892179,  0.23664738],\n",
    "       [ 0.29198961,  0.26234032,  0.16505091],\n",
    "       [ 0.34396535,  0.24206137,  0.16240774]]), indices=array([[3, 0, 5],\n",
    "       [0, 1, 4],\n",
    "       [0, 5, 1],\n",
    "       [1, 3, 5],\n",
    "       [1, 4, 3]], dtype=int32))\n",
    "```\n",
    "\n",
    "Looking just at the first row we get `[ 0.34763842,  0.24879643,  0.12789202]`, you can confirm these are the 3 largest probabilities in `a`. You'll also notice `[3, 0, 5]` are the corresponding indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Print out the top five softmax probabilities for the predictions on the German traffic sign images found on the web. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Project Writeup\n",
    "\n",
    "Once you have completed the code implementation, document your results in a project writeup using this [template](https://github.com/udacity/CarND-Traffic-Sign-Classifier-Project/blob/master/writeup_template.md) as a guide. The writeup can be in a markdown or pdf file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## Step 4 (Optional): Visualize the Neural Network's State with Test Images\n",
    "\n",
    " This Section is not required to complete but acts as an additional excersise for understaning the output of a neural network's weights. While neural networks can be a great learning device they are often referred to as a black box. We can understand what the weights of a neural network look like better by plotting their feature maps. After successfully training your neural network you can see what it's feature maps look like by plotting the output of the network's weight layers in response to a test stimuli image. From these plotted feature maps, it's possible to see what characteristics of an image the network finds interesting. For a sign, maybe the inner network feature maps react with high activation to the sign's boundary outline or to the contrast in the sign's painted symbol.\n",
    "\n",
    " Provided for you below is the function code that allows you to get the visualization output of any tensorflow weight layer you want. The inputs to the function should be a stimuli image, one used during training or a new one you provided, and then the tensorflow variable name that represents the layer's state during the training process, for instance if you wanted to see what the [LeNet lab's](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/601ae704-1035-4287-8b11-e2c2716217ad/concepts/d4aca031-508f-4e0b-b493-e7b706120f81) feature maps looked like for it's second convolutional layer you could enter conv2 as the tf_activation variable.\n",
    "\n",
    "For an example of what feature map outputs look like, check out NVIDIA's results in their paper [End-to-End Deep Learning for Self-Driving Cars](https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/) in the section Visualization of internal CNN State. NVIDIA was able to show that their network's inner weights had high activations to road boundary lines by comparing feature maps from an image with a clear path to one without. Try experimenting with a similar test to show that your trained network's weights are looking for interesting features, whether it's looking at differences in feature maps from images with or without a sign, or even what feature maps look like in a trained network vs a completely untrained one on the same sign image.\n",
    "\n",
    "<figure>\n",
    " <img src=\"visualize_cnn.png\" width=\"380\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Your output should look something like this (above)</p> \n",
    " </figcaption>\n",
    "</figure>\n",
    " <p></p> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Visualize your network's feature maps here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "# image_input: the test image being fed into the network to produce the feature maps\n",
    "# tf_activation: should be a tf variable name used during your training procedure that represents the calculated state of a specific weight layer\n",
    "# activation_min/max: can be used to view the activation contrast in more detail, by default matplot sets min and max to the actual min and max values of the output\n",
    "# plt_num: used to plot out multiple different weight feature map sets on the same block, just extend the plt number for each new feature map entry\n",
    "\n",
    "def outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1 ,plt_num=1):\n",
    "    # Here make sure to preprocess your image_input in a way your network expects\n",
    "    # with size, normalization, ect if needed\n",
    "    # image_input =\n",
    "    # Note: x should be the same name as your network's tensorflow data placeholder variable\n",
    "    # If you get an error tf_activation is not defined it may be having trouble accessing the variable from inside a function\n",
    "    activation = tf_activation.eval(session=sess,feed_dict={x : image_input})\n",
    "    featuremaps = activation.shape[3]\n",
    "    plt.figure(plt_num, figsize=(15,15))\n",
    "    for featuremap in range(featuremaps):\n",
    "        plt.subplot(6,8, featuremap+1) # sets the number of feature maps to show on each row and column\n",
    "        plt.title('FeatureMap ' + str(featuremap)) # displays the feature map number\n",
    "        if activation_min != -1 & activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin =activation_min, vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_min !=-1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin=activation_min, cmap=\"gray\")\n",
    "        else:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
